import os
import emoji
import urllib
import requests
import regex as re
from langdetect import detect

from io import StringIO
from vncorenlp import VnCoreNLP
from transformers import pipeline


# https://ihateregex.io
class VietnameseTextCleaner:
    VN_CHARS = 'Ã¡Ã áº£Ã£áº¡Äƒáº¯áº±áº³áºµáº·Ã¢áº¥áº§áº©áº«áº­Ã©Ã¨áº»áº½áº¹Ãªáº¿á»á»ƒá»…á»‡Ã³Ã²á»Ãµá»Ã´á»‘á»“á»•á»—á»™Æ¡á»›á»á»Ÿá»¡á»£Ã­Ã¬á»‰Ä©á»‹ÃºÃ¹á»§Å©á»¥Æ°á»©á»«á»­á»¯á»±Ã½á»³á»·á»¹á»µÄ‘ÃÃ€áº¢Ãƒáº Ä‚áº®áº°áº²áº´áº¶Ã‚áº¤áº¦áº¨áºªáº¬Ã‰Ãˆáººáº¼áº¸ÃŠáº¾á»€á»‚á»„á»†Ã“Ã’á»Ã•á»ŒÃ”á»á»’á»”á»–á»˜Æ á»šá»œá»á» á»¢ÃÃŒá»ˆÄ¨á»ŠÃšÃ™á»¦Å¨á»¤Æ¯á»¨á»ªá»¬á»®á»°Ãá»²á»¶á»¸á»´Ä'
    
    @staticmethod
    def remove_html(text):
        """
        XÃ³a cÃ¡c tháº» HTML khá»i vÄƒn báº£n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a cÃ¡c tháº» HTML
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xÃ³a cÃ¡c tháº» HTML
        """
        return re.sub(r'<[^>]*>', '', text)
    
    @staticmethod
    def remove_emoji(text):
        """
        XÃ³a cÃ¡c kÃ½ tá»± emoji khá»i vÄƒn báº£n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a emoji
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xÃ³a emoji
        """
        return emoji.replace_emoji(text, '')
    
    @staticmethod
    def remove_url(text):
        """
        XÃ³a cÃ¡c URL khá»i vÄƒn báº£n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a URL
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xÃ³a URL
        """
        return re.sub(r'https?:\/\/(www\.)?[-a-zA-Z0-9@:%._\+~#=]{1,256}\.[a-zA-Z0-9()]{1,6}\b([-a-zA-Z0-9()!@:%_\+.~#?&\/\/=]*)', '', text)
    
    @staticmethod
    def remove_email(text):
        """
        XÃ³a Ä‘á»‹a chá»‰ email khá»i vÄƒn báº£n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a email
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xÃ³a email
        """
        return re.sub(r'[^@ \t\r\n]+@[^@ \t\r\n]+\.[^@ \t\r\n]+', '', text)
    
    @staticmethod
    def remove_phone_number(text):
        """
        XÃ³a sá»‘ Ä‘iá»‡n thoáº¡i khá»i vÄƒn báº£n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a sá»‘ Ä‘iá»‡n thoáº¡i
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xÃ³a sá»‘ Ä‘iá»‡n thoáº¡i
        """
        return re.sub(r'^[\+]?[(]?[0-9]{3}[)]?[-\s\.]?[0-9]{3}[-\s\.]?[0-9]{4,6}$', '', text)
    
    @staticmethod
    def remove_hashtags(text):
        """
        XÃ³a hashtag khá»i vÄƒn báº£n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a hashtag
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ xÃ³a hashtag
        """
        return re.sub(r'#\w+', '', text)
    
    @staticmethod
    def remove_unnecessary_characters(text):
        """
        XÃ³a cÃ¡c kÃ½ tá»± khÃ´ng cáº§n thiáº¿t vÃ  chuáº©n hÃ³a khoáº£ng tráº¯ng
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n lÃ m sáº¡ch
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c lÃ m sáº¡ch vÃ  chuáº©n hÃ³a khoáº£ng tráº¯ng
        """
        text = re.sub(fr"[^\sa-zA-Z0-9{VietnameseTextCleaner.VN_CHARS}]", ' ', text)
        return re.sub(r'\s+', ' ', text).strip() # Remove extra whitespace
    
    @staticmethod
    def process_text(text):
        """
        Xá»­ lÃ½ vÄƒn báº£n báº±ng cÃ¡ch Ã¡p dá»¥ng táº¥t cáº£ cÃ¡c phÆ°Æ¡ng thá»©c lÃ m sáº¡ch
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n xá»­ lÃ½
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½ vÃ  lÃ m sáº¡ch
        """
        text = VietnameseTextCleaner.remove_html(text)
        text = VietnameseTextCleaner.remove_emoji(text)
        text = VietnameseTextCleaner.remove_url(text)
        text = VietnameseTextCleaner.remove_email(text)
        text = VietnameseTextCleaner.remove_phone_number(text)
        text = VietnameseTextCleaner.remove_hashtags(text)
        return VietnameseTextCleaner.remove_unnecessary_characters(text)

    @staticmethod
    def is_english_comment(text):
        """
        Kiá»ƒm tra xem comment cÃ³ pháº£i lÃ  tiáº¿ng Anh khÃ´ng
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n kiá»ƒm tra
        Returns:
            bool: True náº¿u lÃ  tiáº¿ng Anh, False náº¿u khÃ´ng pháº£i
        """
        try:
            return detect(text) == 'en'
        except:
            return False

class VietnameseToneNormalizer:
    VOWELS_TABLE = [
        ['a', 'Ã ', 'Ã¡', 'áº£', 'Ã£', 'áº¡', 'a'],
        ['Äƒ', 'áº±', 'áº¯', 'áº³', 'áºµ', 'áº·', 'aw'],
        ['Ã¢', 'áº§', 'áº¥', 'áº©', 'áº«', 'áº­', 'aa'],
        ['e', 'Ã¨', 'Ã©', 'áº»', 'áº½', 'áº¹', 'e' ],
        ['Ãª', 'á»', 'áº¿', 'á»ƒ', 'á»…', 'á»‡', 'ee'],
        ['i', 'Ã¬', 'Ã­', 'á»‰', 'Ä©', 'á»‹', 'i' ],
        ['o', 'Ã²', 'Ã³', 'á»', 'Ãµ', 'á»', 'o' ],
        ['Ã´', 'á»“', 'á»‘', 'á»•', 'á»—', 'á»™', 'oo'],
        ['Æ¡', 'á»', 'á»›', 'á»Ÿ', 'á»¡', 'á»£', 'ow'],
        ['u', 'Ã¹', 'Ãº', 'á»§', 'Å©', 'á»¥', 'u' ],
        ['Æ°', 'á»«', 'á»©', 'á»­', 'á»¯', 'á»±', 'uw'],
        ['y', 'á»³', 'Ã½', 'á»·', 'á»¹', 'á»µ', 'y']
    ]
    
    VOWELS_TO_IDS = {
        'a': (0, 0), 'Ã ': (0, 1), 'Ã¡': (0, 2), 'áº£': (0, 3), 'Ã£': (0, 4), 'áº¡': (0, 5), 
        'Äƒ': (1, 0), 'áº±': (1, 1), 'áº¯': (1, 2), 'áº³': (1, 3), 'áºµ': (1, 4), 'áº·': (1, 5), 
        'Ã¢': (2, 0), 'áº§': (2, 1), 'áº¥': (2, 2), 'áº©': (2, 3), 'áº«': (2, 4), 'áº­': (2, 5), 
        'e': (3, 0), 'Ã¨': (3, 1), 'Ã©': (3, 2), 'áº»': (3, 3), 'áº½': (3, 4), 'áº¹': (3, 5), 
        'Ãª': (4, 0), 'á»': (4, 1), 'áº¿': (4, 2), 'á»ƒ': (4, 3), 'á»…': (4, 4), 'á»‡': (4, 5), 
        'i': (5, 0), 'Ã¬': (5, 1), 'Ã­': (5, 2), 'á»‰': (5, 3), 'Ä©': (5, 4), 'á»‹': (5, 5), 
        'o': (6, 0), 'Ã²': (6, 1), 'Ã³': (6, 2), 'á»': (6, 3), 'Ãµ': (6, 4), 'á»': (6, 5), 
        'Ã´': (7, 0), 'á»“': (7, 1), 'á»‘': (7, 2), 'á»•': (7, 3), 'á»—': (7, 4), 'á»™': (7, 5), 
        'Æ¡': (8, 0), 'á»': (8, 1), 'á»›': (8, 2), 'á»Ÿ': (8, 3), 'á»¡': (8, 4), 'á»£': (8, 5), 
        'u': (9, 0), 'Ã¹': (9, 1), 'Ãº': (9, 2), 'á»§': (9, 3), 'Å©': (9, 4), 'á»¥': (9, 5), 
        'Æ°': (10, 0), 'á»«': (10, 1), 'á»©': (10, 2), 'á»­': (10, 3), 'á»¯': (10, 4), 'á»±': (10, 5), 
        'y': (11, 0), 'á»³': (11, 1), 'Ã½': (11, 2), 'á»·': (11, 3), 'á»¹': (11, 4), 'á»µ': (11, 5)
    }
    
    VINAI_NORMALIZED_TONE = {
        'Ã²a': 'oÃ ', 'Ã’a': 'OÃ ', 'Ã’A': 'OÃ€', 
        'Ã³a': 'oÃ¡', 'Ã“a': 'OÃ¡', 'Ã“A': 'OÃ', 
        'á»a': 'oáº£', 'á»a': 'Oáº£', 'á»A': 'Oáº¢',
        'Ãµa': 'oÃ£', 'Ã•a': 'OÃ£', 'Ã•A': 'OÃƒ',
        'á»a': 'oáº¡', 'á»Œa': 'Oáº¡', 'á»ŒA': 'Oáº ',
        'Ã²e': 'oÃ¨', 'Ã’e': 'OÃ¨', 'Ã’E': 'OÃˆ',
        'Ã³e': 'oÃ©', 'Ã“e': 'OÃ©', 'Ã“E': 'OÃ‰',
        'á»e': 'oáº»', 'á»e': 'Oáº»', 'á»E': 'Oáºº',
        'Ãµe': 'oáº½', 'Ã•e': 'Oáº½', 'Ã•E': 'Oáº¼',
        'á»e': 'oáº¹', 'á»Œe': 'Oáº¹', 'á»ŒE': 'Oáº¸',
        'Ã¹y': 'uá»³', 'Ã™y': 'Uá»³', 'Ã™Y': 'Uá»²',
        'Ãºy': 'uÃ½', 'Ãšy': 'UÃ½', 'ÃšY': 'UÃ',
        'á»§y': 'uá»·', 'á»¦y': 'Uá»·', 'á»¦Y': 'Uá»¶',
        'Å©y': 'uá»¹', 'Å¨y': 'Uá»¹', 'Å¨Y': 'Uá»¸',
        'á»¥y': 'uá»µ', 'á»¤y': 'Uá»µ', 'á»¤Y': 'Uá»´',
    }

    @staticmethod
    def normalize_unicode(text):
        """
        Chuáº©n hÃ³a Unicode cho vÄƒn báº£n tiáº¿ng Viá»‡t
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n chuáº©n hÃ³a
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a Unicode
        """
        char1252 = r'aÌ€|aÌ|aÌ‰|aÌƒ|aÌ£|Ã¢Ì€|Ã¢Ì|Ã¢Ì‰|Ã¢Ìƒ|Ã¢Ì£|ÄƒÌ€|ÄƒÌ|ÄƒÌ‰|ÄƒÌƒ|ÄƒÌ£|eÌ€|eÌ|eÌ‰|eÌƒ|eÌ£|ÃªÌ€|ÃªÌ|ÃªÌ‰|ÃªÌƒ|ÃªÌ£|iÌ€|iÌ|iÌ‰|iÌƒ|iÌ£|oÌ€|oÌ|oÌ‰|oÌƒ|oÌ£|Ã´Ì€|Ã´Ì|Ã´Ì‰|Ã´Ìƒ|Ã´Ì£|Æ¡Ì€|Æ¡Ì|Æ¡Ì‰|Æ¡Ìƒ|Æ¡Ì£|uÌ€|uÌ|uÌ‰|uÌƒ|uÌ£|Æ°Ì€|Æ°Ì|Æ°Ì‰|Æ°Ìƒ|Æ°Ì£|yÌ€|yÌ|yÌ‰|yÌƒ|yÌ£|AÌ€|AÌ|AÌ‰|AÌƒ|AÌ£|Ã‚Ì€|Ã‚Ì|Ã‚Ì‰|Ã‚Ìƒ|Ã‚Ì£|Ä‚Ì€|Ä‚Ì|Ä‚Ì‰|Ä‚Ìƒ|Ä‚Ì£|EÌ€|EÌ|EÌ‰|EÌƒ|EÌ£|ÃŠÌ€|ÃŠÌ|ÃŠÌ‰|ÃŠÌƒ|ÃŠÌ£|IÌ€|IÌ|IÌ‰|IÌƒ|IÌ£|OÌ€|OÌ|OÌ‰|OÌƒ|OÌ£|Ã”Ì€|Ã”Ì|Ã”Ì‰|Ã”Ìƒ|Ã”Ì£|Æ Ì€|Æ Ì|Æ Ì‰|Æ Ìƒ|Æ Ì£|UÌ€|UÌ|UÌ‰|UÌƒ|UÌ£|Æ¯Ì€|Æ¯Ì|Æ¯Ì‰|Æ¯Ìƒ|Æ¯Ì£|YÌ€|YÌ|YÌ‰|YÌƒ|YÌ£'
        charutf8 = r'Ã |Ã¡|áº£|Ã£|áº¡|áº§|áº¥|áº©|áº«|áº­|áº±|áº¯|áº³|áºµ|áº·|Ã¨|Ã©|áº»|áº½|áº¹|á»|áº¿|á»ƒ|á»…|á»‡|Ã¬|Ã­|á»‰|Ä©|á»‹|Ã²|Ã³|á»|Ãµ|á»|á»“|á»‘|á»•|á»—|á»™|á»|á»›|á»Ÿ|á»¡|á»£|Ã¹|Ãº|á»§|Å©|á»¥|á»«|á»©|á»­|á»¯|á»±|á»³|Ã½|á»·|á»¹|á»µ|Ã€|Ã|áº¢|Ãƒ|áº |áº¦|áº¤|áº¨|áºª|áº¬|áº°|áº®|áº²|áº´|áº¶|Ãˆ|Ã‰|áºº|áº¼|áº¸|á»€|áº¾|á»‚|á»„|á»†|ÃŒ|Ã|á»ˆ|Ä¨|á»Š|Ã’|Ã“|á»|Ã•|á»Œ|á»’|á»|á»”|á»–|á»˜|á»œ|á»š|á»|á» |á»¢|Ã™|Ãš|á»¦|Å¨|á»¤|á»ª|á»¨|á»¬|á»®|á»°|á»²|Ã|á»¶|á»¸|á»´'
        char_map = dict(zip(char1252.split('|'), charutf8.split('|')))
        return re.sub(char1252, lambda x: char_map[x.group()], text.strip())
    
    @staticmethod
    def normalize_sentence_typing(text, vinai_normalization=False):
        """
        Chuáº©n hÃ³a cÃ¡ch gÃµ dáº¥u trong cÃ¢u tiáº¿ng Viá»‡t
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n chuáº©n hÃ³a
            vinai_normalization (bool): CÃ³ sá»­ dá»¥ng chuáº©n hÃ³a theo VINAI khÃ´ng
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a cÃ¡ch gÃµ dáº¥u
        """
        # https://github.com/VinAIResearch/BARTpho/blob/main/VietnameseToneNormalization.md
        if vinai_normalization:
            for wrong, correct in VietnameseToneNormalizer.VINAI_NORMALIZED_TONE.items():
                text = text.replace(wrong, correct)
            return text.strip()
        
        words = text.strip().split()
        for index, word in enumerate(words):
            cw = re.sub(r'(^\p{P}*)([p{L}.]*\p{L}+)(\p{P}*$)', r'\1/\2/\3', word).split('/')
            if len(cw) == 3: cw[1] = VietnameseToneNormalizer.normalize_word_typing(cw[1])
            words[index] = ''.join(cw)
        return ' '.join(words)
    
    @staticmethod
    def normalize_word_typing(word):
        """
        Chuáº©n hÃ³a cÃ¡ch gÃµ dáº¥u trong tá»« tiáº¿ng Viá»‡t
        Args:
            word (str): Tá»« cáº§n chuáº©n hÃ³a
        Returns:
            str: Tá»« Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a cÃ¡ch gÃµ dáº¥u
        """
        if not VietnameseToneNormalizer.is_valid_vietnamese_word(word): return word
        chars, vowel_indexes = list(word), []
        qu_or_gi, tonal_mark = False, 0
        
        for index, char in enumerate(chars):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            row, col = VietnameseToneNormalizer.VOWELS_TO_IDS[char]
            if index > 0 and (row, chars[index - 1]) in [(9, 'q'), (5, 'g')]:
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                qu_or_gi = True
                
            if not qu_or_gi or index != 1: vowel_indexes.append(index)
            if col != 0:
                tonal_mark = col
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][0]
                
        if len(vowel_indexes) < 2:
            if qu_or_gi:
                index = 1 if len(chars) == 2 else 2
                if chars[index] in VietnameseToneNormalizer.VOWELS_TO_IDS:
                    row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
                    chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                else: chars[1] = VietnameseToneNormalizer.VOWELS_TABLE[5 if chars[1] == 'i' else 9][tonal_mark]
                return ''.join(chars)
            return word
        
        for index in vowel_indexes:
            row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
            if row in [4, 8]: # Ãª, Æ¡
                chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
                return ''.join(chars)
            
        index = vowel_indexes[0 if len(vowel_indexes) == 2 and vowel_indexes[-1] == len(chars) - 1 else 1] 
        row, _ = VietnameseToneNormalizer.VOWELS_TO_IDS[chars[index]]
        chars[index] = VietnameseToneNormalizer.VOWELS_TABLE[row][tonal_mark]
        return ''.join(chars)
    
    @staticmethod
    def is_valid_vietnamese_word(word):
        """
        Kiá»ƒm tra xem má»™t tá»« cÃ³ pháº£i lÃ  tá»« tiáº¿ng Viá»‡t há»£p lá»‡ khÃ´ng
        Args:
            word (str): Tá»« cáº§n kiá»ƒm tra
        Returns:
            bool: True náº¿u lÃ  tá»« tiáº¿ng Viá»‡t há»£p lá»‡, False náº¿u khÃ´ng
        """
        vowel_indexes = -1 
        for index, char in enumerate(word):
            if char not in VietnameseToneNormalizer.VOWELS_TO_IDS: continue
            if vowel_indexes in [-1, index - 1]: vowel_indexes = index
            else: return False
        return True

class VietnameseTextPreprocessor:
    def __init__(self, vncorenlp_dir='./VnCoreNLP', extra_teencodes=None, max_correction_length=512):
        """
        Khá»Ÿi táº¡o bá»™ tiá»n xá»­ lÃ½ vÄƒn báº£n tiáº¿ng Viá»‡t
        Args:
            vncorenlp_dir (str): ThÆ° má»¥c chá»©a cÃ¡c file VnCoreNLP
            extra_teencodes (dict): CÃ¡c Ã¡nh xáº¡ teencode bá»• sung
            max_correction_length (int): Äá»™ dÃ i tá»‘i Ä‘a cho viá»‡c sá»­a lá»—i vÄƒn báº£n
        """
        self.vncorenlp_dir = vncorenlp_dir
        self.extra_teencodes = extra_teencodes
        self._load_vncorenlp()
        self._build_teencodes()
        
        self.max_correction_length = max_correction_length
        self.corrector = pipeline(
            'text2text-generation', model='bmd1905/vietnamese-correction-v2', 
            torch_dtype='bfloat16', device_map='auto', num_workers=os.cpu_count()
        )
        print('bmd1905/vietnamese-correction-v2 is loaded successfully.')
        
    
    def _load_vncorenlp(self):
        """
        Táº£i bá»™ phÃ¢n Ä‘oáº¡n tá»« VnCoreNLP
        """
        self.word_segmenter = None
        if self._get_vncorenlp_files('/VnCoreNLP-1.2.jar') and \
           self._get_vncorenlp_files('/models/wordsegmenter/vi-vocab') and \
           self._get_vncorenlp_files('/models/wordsegmenter/wordsegmenter.rdr'):
            self.word_segmenter = VnCoreNLP(self.vncorenlp_dir + '/VnCoreNLP-1.2.jar', annotators='wseg', quiet=False)
            print('VnCoreNLP word segmenter is loaded successfully.')
        else: print('Failed to load VnCoreNLP word segmenter.')
            

    def _get_vncorenlp_files(self, url_slash):
        """
        Táº£i cÃ¡c file VnCoreNLP náº¿u chÃºng chÆ°a tá»“n táº¡i
        Args:
            url_slash (str): ÄÆ°á»ng dáº«n URL Ä‘áº¿n file
        Returns:
            bool: True náº¿u file tá»“n táº¡i hoáº·c táº£i thÃ nh cÃ´ng, False náº¿u khÃ´ng
        """
        local_path = self.vncorenlp_dir + url_slash
        if os.path.exists(local_path): return True
        
        # Check if the folder contains the local_path exists, if not, create it.
        if not os.path.exists(os.path.dirname(local_path)):
            os.makedirs(os.path.dirname(local_path))
        
        download_url = 'https://raw.githubusercontent.com/vncorenlp/VnCoreNLP/master' + url_slash
        try: 
            print(f'Downloading {download_url} to {local_path}')
            return urllib.request.urlretrieve(download_url, local_path)
        except urllib.error.HTTPError as e:
            print(f'Failed to download {download_url} due to {e}')
            return False
                
        
    def _build_teencodes(self):
        """
        XÃ¢y dá»±ng tá»« Ä‘iá»ƒn teencode tá»« cÃ¡c Ã¡nh xáº¡ máº·c Ä‘á»‹nh vÃ  bá»• sung
        """
        self.teencodes = {
            'ok': ['okie', 'okey', 'Ã´kÃª', 'oki', 'oke', 'okay', 'okÃª'], 
            'khÃ´ng': ['kg', 'not', 'k', 'kh', 'kÃ´', 'hok', 'ko', 'khong'], 'khÃ´ng pháº£i': ['kp'], 
            'cáº£m Æ¡n': ['tks', 'thks', 'thanks', 'ths', 'thank'], 'há»“i Ä‘Ã³': ['hÃ¹i Ä‘Ã³'], 'muá»‘n': ['mÃºn'],
            
            'ráº¥t tá»‘t': ['perfect', 'â¤ï¸', 'ğŸ˜'], 'dá»… thÆ°Æ¡ng': ['cute'], 'yÃªu': ['iu'], 'thÃ­ch': ['thik'], 
            'tá»‘t': [
                'gud', 'good', 'gÃºt', 'tot', 'nice',
                'hehe', 'hihi', 'haha', 'hjhj', 'thick', '^_^', ':)', '=)'
                'ğŸ‘', 'ğŸ‰', 'ğŸ˜€', 'ğŸ˜‚', 'ğŸ¤—', 'ğŸ˜™', 'ğŸ™‚'
            ], 
            'bÃ¬nh thÆ°á»ng': ['bt', 'bthg'], 'hÃ g': ['hÃ ng'], 
            'khÃ´ng tá»‘t':  ['lol', 'cc', 'huhu', ':(', 'ğŸ˜”', 'ğŸ˜“'],
            'tá»‡': ['sad', 'por', 'poor', 'bad'], 'giáº£ máº¡o': ['fake'], 
            
            'quÃ¡': ['wa', 'wÃ¡', 'qÃ¡'], 'Ä‘Æ°á»£c': ['Ä‘x', 'dk', 'dc', 'Ä‘k', 'Ä‘c'], 
            'vá»›i': ['vs'], 'gÃ¬': ['j'], 'rá»“i': ['r'], 'mÃ¬nh': ['m', 'mik'], 
            'thá»i gian': ['time'], 'giá»': ['h'], 
        }
        if self.extra_teencodes: 
            for key, values in self.extra_teencodes.items():
                if any(len(value.split()) > 1 for value in values):
                    raise ValueError('The values for each key in extra_teencodes must be single words.')
                self.teencodes.setdefault(key, []).extend(values)
                
        self.teencodes = {word: key for key, values in self.teencodes.items() for word in values}
        teencode_url = 'https://gist.githubusercontent.com/behitek/7d9441c10b3c2739499fc5a4d9ea06fb/raw/df939245b3e841b62af115be4dcb3516dadc9fc5/teencode.txt'
        response = requests.get(teencode_url)
        
        if response.status_code == 200:
            text_data = StringIO(response.text)
            for pair in text_data:
                teencode, true_text = pair.split('\t')
                self.teencodes[teencode.strip()] = true_text.strip()
            self.teencodes = {k: self.teencodes[k] for k in sorted(self.teencodes)}
        else: print('Failed to fetch teencode.txt from', teencode_url)

    
    def normalize_teencodes(self, text):
        """
        Chuáº©n hÃ³a cÃ¡c teencode trong vÄƒn báº£n vá» dáº¡ng chuáº©n
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o chá»©a teencode
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c chuáº©n hÃ³a teencode
        """
        words = []
        for word in text.split():
            words.append(self.teencodes.get(word, word))
        return ' '.join(words)
    
    # https://huggingface.co/bmd1905/vietnamese-correction-v2
    def correct_vietnamese_errors(self, texts):
        """
        Sá»­a lá»—i chÃ­nh táº£ vÃ  ngá»¯ phÃ¡p tiáº¿ng Viá»‡t
        Args:
            texts (list): Danh sÃ¡ch cÃ¡c vÄƒn báº£n cáº§n sá»­a
        Returns:
            list: Danh sÃ¡ch cÃ¡c vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c sá»­a
        """
        predictions = self.corrector(texts, max_length=self.max_correction_length, truncation=True)
        return [prediction['generated_text'] for prediction in predictions]
        
    
    def word_segment(self, text):
        """
        Thá»±c hiá»‡n phÃ¢n Ä‘oáº¡n tá»« tiáº¿ng Viá»‡t
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n phÃ¢n Ä‘oáº¡n
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c phÃ¢n Ä‘oáº¡n
        """
        if self.word_segmenter: 
            words = self.word_segmenter.tokenize(text)
            return ' '.join(sum(words, []))
        print('KhÃ´ng cÃ³ trÃ¬nh phÃ¢n Ä‘oáº¡n tá»« VnCoreNLP nÃ o Ä‘Æ°á»£c táº£i. Vui lÃ²ng kiá»ƒm tra tá»‡p jar VnCoreNLP.')
        return text
        
    
    def process_text(self, text, normalize_tone=True, segment=True):
        """
        Xá»­ lÃ½ vÄƒn báº£n vá»›i cÃ¡c bÆ°á»›c chuáº©n hÃ³a khÃ¡c nhau
        Args:
            text (str): VÄƒn báº£n Ä‘áº§u vÃ o cáº§n xá»­ lÃ½
            normalize_tone (bool): CÃ³ chuáº©n hÃ³a dáº¥u tiáº¿ng Viá»‡t khÃ´ng
            segment (bool): CÃ³ thá»±c hiá»‡n phÃ¢n Ä‘oáº¡n tá»« khÃ´ng
        Returns:
            str: VÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
        """
        text = text.lower()
        if normalize_tone:
            text = VietnameseToneNormalizer.normalize_unicode(text)
            text = VietnameseToneNormalizer.normalize_sentence_typing(text)
        text = VietnameseTextCleaner.process_text(text)
        text = self.normalize_teencodes(text)
        return self.word_segment(text) if segment else text
    
    
    def process_batch(self, texts, correct_errors=True):
        """
        Xá»­ lÃ½ má»™t loáº¡t cÃ¡c vÄƒn báº£n
        Args:
            texts (list): Danh sÃ¡ch cÃ¡c vÄƒn báº£n cáº§n xá»­ lÃ½
            correct_errors (bool): CÃ³ sá»­a lá»—i khÃ´ng
        Returns:
            list: Danh sÃ¡ch cÃ¡c vÄƒn báº£n Ä‘Ã£ Ä‘Æ°á»£c xá»­ lÃ½
        """
        if correct_errors:
            texts = [self.process_text(text, normalize_tone=True, segment=False) for text in texts]
            texts = self.correct_vietnamese_errors(texts)
            return [self.process_text(text, normalize_tone=False, segment=True) for text in texts]
        return [self.process_text(text, normalize_tone=True, segment=True) for text in texts]
    
    
    def close_vncorenlp(self):
        """
        ÄÃ³ng bá»™ phÃ¢n Ä‘oáº¡n tá»« VnCoreNLP
        """
        if self.word_segmenter: 
            print('Äang Ä‘Ã³ng trÃ¬nh phÃ¢n Ä‘oáº¡n tá»« VnCoreNLP...')
            self.word_segmenter.close()
    

if __name__ == '__main__':
    extra_teencodes = { 
        'khÃ¡ch sáº¡n': ['ks'], 'nhÃ  hÃ ng': ['nhahang'], 'nhÃ¢n viÃªn': ['nv'],
        'cá»­a hÃ ng': ['store', 'sop', 'shopE', 'shop'], 
        'sáº£n pháº©m': ['sp', 'product'], 'hÃ ng': ['hÃ g'],
        'giao hÃ ng': ['ship', 'delivery', 'sÃ­p'], 'Ä‘áº·t hÃ ng': ['order'], 
        'chuáº©n chÃ­nh hÃ£ng': ['authentic', 'aut', 'auth'], 'háº¡n sá»­ dá»¥ng': ['date', 'hsd'],
        'Ä‘iá»‡n thoáº¡i': ['dt'],  'facebook': ['fb', 'face'],  
        'nháº¯n tin': ['nt', 'ib'], 'tráº£ lá»i': ['tl', 'trl', 'rep'], 
        'feedback': ['fback', 'fedback'], 'sá»­ dá»¥ng': ['sd'], 'xÃ i': ['sÃ i'], 
    }
    
    preprocessor = VietnameseTextPreprocessor(vncorenlp_dir='./VnCoreNLP', extra_teencodes=extra_teencodes, max_correction_length=512)
    sample_texts = [
        'Ga giÆ°Æ¡Ì€ng khÃ´ng saÌ£ch, nhÃ¢n viÃªn quÃªn doÌ£n phoÌ€ng mÃ´Ì£t ngaÌ€y. Cháº¥t lá»±Æ¡ng "ko" Ä‘c thá»ai mÃ¡i ğŸ˜”',
        'CÃ¡m Æ¡n Chudu24 ráº¥t nhiá»uGia Ä‘Ã¬nh tÃ´i cÃ³ 1 ká»³ nghá»‰ vui váº».Resort BÃ¬nh Minh náº±m á»Ÿ vá»‹ trÃ­ ráº¥t Ä‘áº¹p, theo Ä‘Ãºng tiÃªu chuáº©n, cÃ²n vá» Äƒn sÃ¡ng thÃ¬ wa dá»Ÿ, chá»‰ cÃ³ 2,3 mÃ³n Ä‘á»ƒ chá»n',
        'GiÃ¡ cáº£ há»£p lÃ­Ä‚n uá»‘ng thoáº£ thÃ­chGiá»¯ xe miá»…n phÃ­KhÃ´ng gian bá» kÃ¨ thoÃ¡ng mÃ¡t CÃ³ phÃ²ng mÃ¡y láº¡nhMá»—i tá»™i lÃºc quÃ¡n Ä‘Ã´ng thÃ¬ Ä‘á»£i hÆ¡i lÃ¢u',
        'May láº§n trÆ°á»›c Äƒn mÃ¬ k hÃ , hÃ´m nay Äƒn thá»­ bÃºn báº¯p bÃ². CÃ³ cháº£ tÃ´m viÃªn Äƒn láº¡ láº¡. TÃ´m thÃ¬ k nhiá»u, nhÆ°ng váº«n cÃ³ tÃ´m tháº­t á»Ÿ nhÃ¢n bÃªn trong. ',
        'Ngá»“i Äƒn CÆ¡m nhÃ  *tiá»n thÃ¢n lÃ  quÃ¡n BÃ£o* Pháº§n váº­y lÃ  59k nha. TrÆ°a tá»« 10h-14h, chiá»u tá»« 16h-19h. Ã€,cÃ³ sá»¯a háº¡t sen ngon láº¯mm. #food #foodpic #foodporn #foodholic #yummy #deliciuous',
        'This is an English comment with a URL https://example.com',
        'Just another English comment without URL'
    ]
    
    preprocessed_texts = preprocessor.process_batch(sample_texts, correct_errors=True)
    preprocessor.close_vncorenlp()
    print("\nPreprocessed texts:")
    print(preprocessed_texts)